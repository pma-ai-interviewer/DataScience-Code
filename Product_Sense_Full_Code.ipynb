{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkZABZ9KHVkj",
        "outputId": "376cecf6-056f-4753-829a-d08332c516d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.49.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n",
            "Downloading anthropic-0.49.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import anthropic\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Set up Claude API Key\n",
        "client = anthropic.Anthropic(api_key=\"\")\n",
        "\n",
        "def generate_product_sense_question():\n",
        "    \"\"\"\n",
        "    Automatically generates a Product Sense question while incorporating the CIRCLE framework.\n",
        "\n",
        "    AI dynamically selects:\n",
        "    - The product domain (General, Industry-Specific, or Trend-Based)\n",
        "    - The type of product sense question (Problem-Solving, User-Centric, Strategy, or Innovation)\n",
        "    - Ensures candidates apply the CIRCLE framework in their response.\n",
        "\n",
        "    :return: (str) AI-generated Product Sense interview question.\n",
        "    \"\"\"\n",
        "\n",
        "    # AI-driven dynamic question prompt with CIRCLE framework\n",
        "    prompt = \"\"\"\n",
        "    You are an interviewer in a product manager job interview. Generate a **single, brief, high-quality Product Sense interview question**\n",
        "    that feels natural and human-like. The question must encourage candidates to apply the **CIRCLE framework** in their response.\n",
        "\n",
        "    **Definition of Product Sense:**\n",
        "    A case study that tests the interviewee‚Äôs ability to design a product with minimal information, demonstrating\n",
        "    critical thinking and communication skills.\n",
        "\n",
        "    **CIRCLE Framework:**\n",
        "    The question should require the candidate to:\n",
        "    1.**Clarify the Goal** ‚Äì Identify the problem space and define success metrics.\n",
        "    2.**Identify Customer Personas** ‚Äì Define the key users and their needs.\n",
        "    3.**Report Customer Needs** ‚Äì Articulate pain points and key requirements.\n",
        "    4.**Conduct Competitive Analysis** ‚Äì Assess existing solutions and market landscape.\n",
        "    5.**List Solutions** ‚Äì Generate possible product solutions.\n",
        "    6.**Evaluate Trade-offs & Recommend** ‚Äì Prioritize and justify the best solution.\n",
        "\n",
        "    **Rules for the Question:**\n",
        "    - AI should **automatically select** a relevant question type:\n",
        "        - **Problem-Solving:** \"How would you improve X?\"\n",
        "        - **User-Centric:** \"How would you design a product for Y persona?\"\n",
        "        - **Strategy-Oriented:** \"How would you prioritize Z features?\"\n",
        "        - **Innovation-Based:** \"Create a new product for X market.\"\n",
        "    - The question must be **open-ended** to allow structured thinking.\n",
        "\n",
        "    **Examples:**\n",
        "    - Improve AirBnB ‚Äì How would you enhance the booking experience for international travelers?\n",
        "    - Design a mental wellness app for healthcare workers ‚Äì How would you help frontline medical staff manage stress?\n",
        "    - Redesign airport security ‚Äì How would you improve TSA check-in for a smoother experience?\n",
        "    - Create a competitor for IMDb ‚Äì How would you build a next-gen movie discovery platform?\n",
        "\n",
        "    **Generate only ONE brief and well-structured Product Sense interview question that requires the candidate to apply CIRCLE:**\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            max_tokens=100,\n",
        "            system=\"You generate only one structured and high-quality Product Sense interview question, ensuring the candidate applies the CIRCLE framework.\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "\n",
        "        return response.content[0].text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è API Error: {e}\")  #  Debugging output\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# Stores previous Q&A for each session\n",
        "conversation_history = {}\n",
        "\n",
        "# Quality thresholds\n",
        "QUALITY_THRESHOLD = 0.6  # Minimum similarity score for relevance\n",
        "MIN_QUALITY_SCORE = 6  # Minimum score for an AI-generated question\n",
        "\n",
        "\n",
        "def feedback_info_final(session_id):\n",
        "    \"\"\" Generates structured feedback on the entire conversation based on the modified CIRCLE framework. \"\"\"\n",
        "    if session_id not in conversation_history or len(conversation_history[session_id]) == 0:\n",
        "        raise ValueError(\"No conversation history found for this session.\")\n",
        "\n",
        "    # Extract the entire conversation history\n",
        "    conversation = conversation_history[session_id]\n",
        "    formatted_conversation = \"\\n\\n\".join(\n",
        "        [f\"**Q:** {entry['question']}\\n**A:** {entry['answer']}\" for entry in conversation]\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Evaluate the candidate's product sense interview performance using the entire conversation history.\n",
        "\n",
        "    **Conversation History:**\n",
        "    {formatted_conversation}\n",
        "\n",
        "    **Evaluation Criteria (CIRCLE Framework):**\n",
        "    - **Comprehend the Question**: Did the candidate ask clarifying questions and understand the mission?\n",
        "    - **Identify the Customer**: Did they recognize a two-sided marketplace (e.g., consumers and producers)?\n",
        "    - **Cut/Prioritization (Customer Segmentation)**: Did they prioritize a specific customer segment with clear reasoning?\n",
        "    - **Report Customer Needs/Pain Points**: Did they outline at least three key pain points?\n",
        "    - **Cut/Prioritization (Pain Points)**: Did they prioritize one pain point and justify it?\n",
        "    - **List Solutions**: Did they propose at least three solutions, including a moonshot idea?\n",
        "    - **Cut/Prioritization (Solutions)**: Did they prioritize one solution and explain why?\n",
        "    - **Estimate Trade-Offs**: Did they analyze trade-offs between solutions?\n",
        "    - **Summarize**: Did they provide a concise, structured summary?\n",
        "\n",
        "    **Additional Evaluation Metrics:**\n",
        "    - **Clarity and Conciseness**: Was the response structured and free from unnecessary details?\n",
        "    - **Relevance**: Did the response directly address the question and showcase skills?\n",
        "    - **Problem-Solving**: Did the candidate demonstrate effective challenge resolution?\n",
        "    - **Customer Focus**: Did the response show a deep understanding of user needs?\n",
        "\n",
        "    **Final Output Format:**\n",
        "    - **Evaluation Summary**\n",
        "    - **What Went Well** (4-5 strengths)\n",
        "    - **What Can Be Improved** (3-4 areas for improvement)\n",
        "    - **Next Steps** (Actionable improvement suggestions)\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            max_tokens=2048,\n",
        "            system=\"Generate structured interview feedback using the modified CIRCLE framework for the entire conversation.\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "\n",
        "        return response.content[0].text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating final feedback: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def quality_control_check(question_text, answer_text, ai_response):\n",
        "    \"\"\" Ensures AI-generated follow-up questions meet quality standards. \"\"\"\n",
        "    try:\n",
        "        # Step 1: Contextual Relevance Check\n",
        "        relevance_prompt = f\"\"\"\n",
        "        Analyze the following follow-up question for contextual relevance.\n",
        "\n",
        "        Candidate's Response: {answer_text}\n",
        "        AI-Generated Follow-Up Question: {ai_response}\n",
        "\n",
        "        On a scale from **1 to 10**, how well does this follow-up question logically relate to the candidate's response?\n",
        "        (A score of 1 means it is completely unrelated, 10 means it is perfectly relevant.)\n",
        "\n",
        "        Score (just a number, no explanation):\n",
        "        \"\"\"\n",
        "\n",
        "        validation_response = client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            max_tokens=100,\n",
        "            system=\"You verify AI responses for relevance.\",\n",
        "            messages=[{\"role\": \"user\", \"content\": relevance_prompt}],\n",
        "        )\n",
        "\n",
        "        score_text = validation_response.content[0].text.strip()\n",
        "        score_match = re.search(r\"\\d+\", score_text)\n",
        "        relevance_score = int(score_match.group()) if score_match else 5  # Default to 5 if extraction fails\n",
        "\n",
        "        if relevance_score < 6:\n",
        "            print(\"Warning: AI-generated question may not be contextually relevant!\")\n",
        "            return {\"valid\": False, \"reason\": f\"Low relevance score ({relevance_score}/10).\"}\n",
        "\n",
        "        # Step 2: Scoring System for Response Quality\n",
        "        scoring_prompt = f\"\"\"\n",
        "        Evaluate the quality of the AI-generated follow-up question based on clarity, relevance, and depth.\n",
        "        Give a score from 1 to 10, with **ONLY the number** as the response.\n",
        "\n",
        "        Question: {question_text}\n",
        "        Candidate Response: {answer_text}\n",
        "        AI-Generated Follow-Up Question: {ai_response}\n",
        "\n",
        "        Score (just a number, no explanation):\n",
        "        \"\"\"\n",
        "\n",
        "        scoring_response = client.messages.create(\n",
        "            model=\"claude-3-7-sonnet-20250219\",\n",
        "            max_tokens=100,\n",
        "            system=\"You score AI responses for clarity and quality.\",\n",
        "            messages=[{\"role\": \"user\", \"content\": scoring_prompt}],\n",
        "        )\n",
        "\n",
        "        score_text = scoring_response.content[0].text.strip()\n",
        "        score_match = re.search(r\"\\d+\", score_text)\n",
        "        score = int(score_match.group()) if score_match else 5  # Default to 5 if extraction fails\n",
        "\n",
        "        if score < MIN_QUALITY_SCORE:\n",
        "            print(f\"‚ö† Warning: AI-generated question scored low (score: {score}/10).\")\n",
        "            return {\"valid\": False, \"reason\": f\"Low-quality response (score: {score}/10).\"}\n",
        "\n",
        "        return {\"valid\": True, \"reason\": \"AI-generated follow-up question is high quality.\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in quality control check: {e}\")\n",
        "        return {\"valid\": False, \"reason\": \"Quality control system failed.\"}\n",
        "\n",
        "\n",
        "def generate_follow_up_question(session_id):\n",
        "    \"\"\" Generates dynamic follow-up questions with quality control. \"\"\"\n",
        "    if session_id not in conversation_history or len(conversation_history[session_id]) == 0:\n",
        "        raise ValueError(\"No conversation history found for session.\")\n",
        "\n",
        "    history = conversation_history[session_id]\n",
        "\n",
        "    new_question = None\n",
        "    is_valid = False\n",
        "    attempts = 0\n",
        "    while not is_valid and attempts < 5:  # Allow 5 attempts\n",
        "        attempts += 1\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a professional interviewer conducting a product management interview.\n",
        "        Your goal is to generate a **natural and engaging** follow-up question while ensuring a dynamic, non-linear conversation.\n",
        "\n",
        "        ### **Conversation Context**\n",
        "        - **Original Interview Question:** \"{history[0]['question']}\"\n",
        "        - **Last Question Asked:** \"{history[-1]['question']}\"\n",
        "        - **Candidate‚Äôs Last Response:** \"{history[-1]['answer']}\"\n",
        "        - **Previous Related Answers:** { [entry['answer'] for entry in history] }\n",
        "\n",
        "        Now, based on the conversation flow, generate only one well-structured and engaging follow-up question at a time.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = client.messages.create(\n",
        "                model=\"claude-3-7-sonnet-20250219\",\n",
        "                max_tokens=150,\n",
        "                system=\"You generate high-quality follow-up questions.\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            )\n",
        "            new_question = response.content[0].text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating follow-up question: {e}\")\n",
        "            return \"Sorry, an error occurred.\"\n",
        "\n",
        "        check = quality_control_check(history[-1][\"question\"], history[-1][\"answer\"], new_question)\n",
        "        is_valid = check[\"valid\"]\n",
        "\n",
        "    return new_question\n",
        "\n",
        "\n",
        "#  Example Interview Session\n",
        "session_id = \"session_123\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Generate Product Sense Question\n",
        "    product_question = generate_product_sense_question()\n",
        "    print(f\"\\n Product Sense Question: {product_question}\")\n",
        "\n",
        "    # Step 2: Get Candidate Answer\n",
        "    candidate_answer = input(\"\\n Enter candidate's answer: \")\n",
        "\n",
        "    # Ensure session exists\n",
        "    if session_id not in conversation_history:\n",
        "        conversation_history[session_id] = []\n",
        "\n",
        "    # Store the first question-answer pair\n",
        "    conversation_history[session_id].append({\"question\": product_question, \"answer\": candidate_answer})\n",
        "\n",
        "    # Step 3: Generate Follow-up Questions\n",
        "    for i in range(3):  # Generate 3 follow-up questions\n",
        "        follow_up_question = generate_follow_up_question(session_id)\n",
        "        print(f\"\\n Follow-Up Question {i + 1}: {follow_up_question}\")\n",
        "\n",
        "        # Take user input as the candidate's answer\n",
        "        candidate_answer = input(\"\\n Enter candidate's answer: \")\n",
        "\n",
        "        # Store question-answer pair in history\n",
        "        conversation_history[session_id].append({\"question\": follow_up_question, \"answer\": candidate_answer})\n",
        "\n",
        "    # Step 4: Generate Final Feedback After Entire Interview\n",
        "    final_feedback = feedback_info_final(session_id)\n",
        "    print(\"\\n Final Interview Feedback:\")\n",
        "    print(final_feedback)\n"
      ],
      "metadata": {
        "id": "47rNpGqGHXab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ff0838-cf35-4895-ccfd-3d6e2781c670"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " üìù Product Sense Question: \"Amazon's customer service department is looking to reduce average call resolution time without compromising customer satisfaction. How would you design a tool for customer service representatives that helps them resolve customer issues more efficiently while maintaining high-quality interactions?\"\n",
            "\n",
            "üó£Ô∏è Enter candidate's answer: I‚Äôd design Amazon SmartAssist, an AI-powered tool that provides real-time resolution suggestions, automated customer history summaries, and workflow automation to help service reps resolve issues faster. It would use NLP for smart search, sentiment analysis for tone guidance, and predictive assistance to recommend the best solutions. Routine tasks like refunds and status updates would be automated, while a live knowledge-sharing hub enables quick escalations. Success metrics would focus on reducing handling time, improving first-contact resolution, and maintaining high customer satisfaction (CSAT) scores.\n",
            "\n",
            " üîÑ Follow-Up Question 1: Tell me more about your approach to balancing automation with the human touch. What specific customer issues would you leave entirely to human representatives, and how would your SmartAssist tool know when to step back versus when to provide more automated assistance?\n",
            "\n",
            "üó£Ô∏è Enter candidate's answer: SmartAssist automates routine tasks like refunds and FAQs while escalating complex or emotional issues (e.g., fraud disputes) using sentiment analysis. It ensures seamless human handoff with issue summaries, balancing efficiency with empathy.\n",
            "\n",
            " üîÑ Follow-Up Question 2: Your ideas around AI and automation are promising. I'd like to shift our focus to the implementation and adoption side. How would you roll out SmartAssist to the customer service team to ensure high adoption rates? What potential resistance might you encounter from representatives, and what specific strategies would you employ to measure whether the tool is actually improving both efficiency metrics and customer satisfaction scores?\n",
            "\n",
            "üó£Ô∏è Enter candidate's answer: I‚Äôd roll out SmartAssist in phases, starting with a pilot group, gathering feedback, and refining before full deployment. Training with real-time demos and gradual integration would ease adoption. Resistance may come from fear of job replacement or workflow disruption, so I‚Äôd emphasize AI as a support tool, not a replacement, and highlight efficiency benefits. Success would be measured by AHT reduction, FCR improvement, and CSAT trends, with A/B testing comparing AI-assisted vs. non-AI-assisted interactions.\n",
            "\n",
            " üîÑ Follow-Up Question 3: Your approach to the SmartAssist rollout is thoughtful. I'd like to explore the longer-term evolution of this tool. Assuming SmartAssist has been successfully implemented and is delivering initial results, what would your roadmap look like for the next 12-18 months? How would you prioritize new features or capabilities based on what you've learned, and how might you expand the tool's impact beyond just reducing call resolution times to create additional value for Amazon's customer service ecosystem?\n",
            "\n",
            "üó£Ô∏è Enter candidate's answer: In 12-18 months, I‚Äôd expand SmartAssist with AI coaching, multilingual support, chat/voice integration, and proactive issue resolution to enhance efficiency. A customer sentiment dashboard would provide insights for better training and service improvements, driving long-term value beyond faster resolutions.\n",
            "\n",
            " üìä Final Interview Feedback:\n",
            "# Evaluation of Candidate's Product Design Performance\n",
            "\n",
            "## Evaluation Summary\n",
            "\n",
            "The candidate demonstrated strong product sense when designing an AI-powered tool (SmartAssist) to help Amazon customer service representatives reduce call resolution times while maintaining satisfaction. They showed good understanding of the problem space, balanced technical solutions with human considerations, and outlined a thoughtful implementation plan with clear metrics for success. The candidate effectively addressed followup questions about balancing automation with human touch, implementation strategy, and future roadmap planning, showing adaptability and strategic thinking.\n",
            "\n",
            "## What Went Well\n",
            "\n",
            "1. **Strong Problem Framing**: The candidate quickly grasped the core challenge of balancing efficiency with customer satisfaction, demonstrating good comprehension of the question without needing clarifying questions.\n",
            "\n",
            "2. **Customer-Centric Approach**: Recognized both customer service representatives and end customers as stakeholders, incorporating needs of both groups into the solution design.\n",
            "\n",
            "3. **Comprehensive Solution Design**: Proposed a well-rounded solution with multiple integrated features (NLP search, sentiment analysis, predictive assistance, workflow automation) that directly addressed the efficiency challenge.\n",
            "\n",
            "4. **Implementation Planning**: Outlined a phased rollout strategy with pilot testing, feedback loops, and change management considerations, showing practical thinking about adoption challenges.\n",
            "\n",
            "5. **Success Metrics**: Clearly defined measurable outcomes (AHT reduction, FCR improvement, CSAT scores) and suggested A/B testing methodologies to validate the solution's effectiveness.\n",
            "\n",
            "## What Can Be Improved\n",
            "\n",
            "1. **Deeper Customer Segmentation**: Could have explored different types of customer service scenarios or representative roles that might need different configurations of the tool.\n",
            "\n",
            "2. **Quantitative Goals**: While success metrics were identified, specific targets (e.g., \"reduce AHT by 15%\") were not established to define what success looks like.\n",
            "\n",
            "3. **Technical Feasibility**: Limited discussion of technical requirements, potential integration challenges with existing systems, or data privacy considerations.\n",
            "\n",
            "4. **Exploring Alternatives**: Could have presented and compared alternative approaches to the problem beyond the single AI-powered solution.\n",
            "\n",
            "## Next Steps\n",
            "\n",
            "1. **Practice Customer Segmentation**: When approaching product design challenges, spend more time identifying distinct user segments and their unique needs before developing solutions.\n",
            "\n",
            "2. **Set Quantitative Goals**: For future case studies, establish specific, measurable targets that define success and allow for clear evaluation of solution effectiveness.\n",
            "\n",
            "3. **Consider Technical Constraints**: Incorporate discussion of technical feasibility, integration requirements, and potential implementation challenges in your solution design.\n",
            "\n",
            "4. **Develop Framework for Alternatives**: Practice generating multiple distinct solution approaches for the same problem, then comparing them against each other using consistent criteria before selecting and refining a final recommendation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S20ZaKkPVKw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}